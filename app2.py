import os
import shutil
import streamlit as st

from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda

# -------------------------------
# í™˜ê²½ ì„¤ì •
# -------------------------------
load_dotenv(".env")

VECTORSTORE_DIR = "faiss_index"
MAX_DOCS = 5  # ìµœëŒ€ ì²¨ë¶€ ê°€ëŠ¥ ë¬¸ì„œ ìˆ˜


# -------------------------------
# ìœ í‹¸ í•¨ìˆ˜ë“¤
# -------------------------------
def load_and_split_docs(uploaded_file):
    """
    ì—…ë¡œë“œëœ PDF/TXTë¥¼ ë¡œì»¬ì— ì €ì¥ í›„ LangChain Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³ ,
    RecursiveCharacterTextSplitterë¡œ chunk ë‹¨ìœ„ë¡œ ë¶„í• í•œë‹¤.
    """
    with open(uploaded_file.name, "wb") as f:
        f.write(uploaded_file.getbuffer())

    if uploaded_file.name.endswith(".pdf"):
        loader = PyPDFLoader(uploaded_file.name)
    else:
        loader = TextLoader(uploaded_file.name, encoding="utf-8")

    documents = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    docs = splitter.split_documents(documents)

    # ê° chunkì— source(íŒŒì¼ëª…) ë©”íƒ€ë°ì´í„° ì§€ì •
    for d in docs:
        d.metadata["source"] = uploaded_file.name

    return docs


def create_vectorstore(docs):
    """
    ì£¼ì–´ì§„ ë¬¸ì„œë“¤ë¡œ ìƒˆ FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ê³  ë¡œì»¬ì— ì €ì¥í•œë‹¤.
    (ì´ì „ ì¸ë±ìŠ¤ëŠ” ë°–ì—ì„œ ì´ë¯¸ ì‚­ì œí–ˆë‹¤ê³  ê°€ì •)
    """
    embeddings = OpenAIEmbeddings()
    vectordb = FAISS.from_documents(docs, embeddings)
    vectordb.save_local(VECTORSTORE_DIR)
    return vectordb


def build_rag_chain(vectordb, task_mode: str, active_sources):
    """
    ê³¼ì œ/ë ˆí¬íŠ¸ ë„ìš°ë¯¸ìš© RAG ì²´ì¸.
    active_sources: ì‚¬ìš©í•  ë¬¸ì„œ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
    """
    base_instruction = """
    ë„ˆëŠ” ì—…ë¡œë“œëœ ë¬¸ì„œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ê³¼ì œì™€ ë ˆí¬íŠ¸ ì‘ì„±ì„ ë„ì™€ì£¼ëŠ” AI ì¡°êµì•¼.
    í•­ìƒ ë¬¸ì„œ ë‚´ìš©ì„ ìµœìš°ì„ ìœ¼ë¡œ ì°¸ê³ í•´ì„œ ë‹µí•´ì•¼ í•˜ê³ ,
    ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì´ë¼ ì¼ë°˜ì ì¸ ì„¤ëª…ì„ í• ê²Œ."ë¼ê³  ë¨¼ì € ì•Œë ¤ì¤€ ë’¤ ì„¤ëª…í•´.
    """

    if task_mode == "ë¬¸ì„œ ìš”ì•½":
        task_instruction = """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ì§€ì •í•œ ë²”ìœ„ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¬¸ì„œ ë‚´ìš©ì„ 3~7ë¬¸ì¥ ì •ë„ë¡œ ìš”ì•½í•´ì¤˜.
        ì¤‘ìš” ê°œë…, í•µì‹¬ ì£¼ì¥, ê²°ë¡ ì´ ë¹ ì§€ì§€ ì•Šê²Œ ì •ë¦¬í•´.
        """
    elif task_mode == "ë ˆí¬íŠ¸ ëª©ì°¨ ì„¤ê³„":
        task_instruction = """
        ì´ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ A4 3~5ì¥ ë¶„ëŸ‰ì˜ ë ˆí¬íŠ¸ ëª©ì°¨ë¥¼ ì„¤ê³„í•´ì¤˜.
        1, 1-1, 1-2 ì™€ ê°™ì€ ê³„ì¸µ êµ¬ì¡°ë¡œ ì‘ì„±í•˜ê³ ,
        ê° ì†Œì œëª© ì˜†ì— í•œ ì¤„ì”© ê·¸ ë¶€ë¶„ì— ì“°ë©´ ì¢‹ì„ ë‚´ìš©ì„ ì„¤ëª…í•´ì¤˜.
        """
    elif task_mode == "í•µì‹¬ ë‚´ìš© ì •ë¦¬":
        task_instruction = """
        ì´ ë¬¸ì„œì—ì„œ ì‚¬ìš©ìê°€ ê¶ê¸ˆí•´í•˜ëŠ” ì£¼ì œì™€ ê´€ë ¨ëœ í•µì‹¬ ê°œë…, ì£¼ì¥, ê·¼ê±°ë¥¼
        bullet ëª©ë¡ í˜•íƒœë¡œ ì§§ê³  ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì¤˜.
        """
    elif task_mode == "ì˜ˆìƒ ì‹œí—˜ë¬¸ì œ ìƒì„±":
        task_instruction = """
        ì´ ë¬¸ì„œë¥¼ ê³µë¶€í•˜ëŠ” í•™ìƒì—ê²Œ ì‹œí—˜ì´ë‚˜ êµ¬ë‘ ë°œí‘œì—ì„œ ë‚˜ì˜¬ ë²•í•œ ë¬¸ì œë¥¼ 3~5ê°œ ë§Œë“¤ì–´ì¤˜.
        ì„œìˆ í˜•, ê°ê´€ì‹, ë…¼ìˆ í˜• ë“±ì„ ì„ì–´ë„ ì¢‹ê³ ,
        ê° ë¬¸ì œë§ˆë‹¤ ëª¨ë²”ë‹µì•ˆì˜ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ 2~3ì¤„ ì •ë„ë¡œ ì œì‹œí•´ì¤˜.
        """
    else:  # "ììœ  ì§ˆì˜ì‘ë‹µ"
        task_instruction = """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µí•˜ë˜,
        ë°˜ë“œì‹œ ë¬¸ì„œì—ì„œ ê·¼ê±°ê°€ ë˜ëŠ” ë‚´ìš© ìœ„ì£¼ë¡œ ì„¤ëª…í•´ì¤˜.
        """

    prompt = ChatPromptTemplate.from_template(
        base_instruction
        + "\n\n[ì‘ì—… ì§€ì¹¨]\n"
        + task_instruction
        + """
        
        [ì‚¬ìš©ì ì§ˆë¬¸]
        {question}

        [ì°¸ê³  ë¬¸ì„œ ë‚´ìš©]
        {context}
        """
    )

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)

    def get_context_from_sources(inputs):
        question = inputs["question"]
        docs = vectordb.similarity_search(question, k=12)

        # active_sourcesê°€ ì§€ì •ë˜ë©´ í•´ë‹¹ sourceë§Œ í•„í„°ë§
        if active_sources:
            docs = [d for d in docs if d.metadata.get("source") in active_sources]

        if not docs:
            return "ì„ íƒí•œ ë¬¸ì„œë“¤ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        return "\n\n".join([d.page_content for d in docs])

    rag_chain = (
        {
            "context": RunnableLambda(get_context_from_sources),
            "question": RunnableLambda(lambda x: x["question"]),
        }
        | prompt
        | llm
    )
    return rag_chain


# -------------------------------
# Streamlit UI
# -------------------------------
st.set_page_config(page_title="ê³¼ì œÂ·ë ˆí¬íŠ¸ ë„ìš°ë¯¸ RAG ì±—ë´‡")
st.title("ğŸ“š ê³¼ì œÂ·ë ˆí¬íŠ¸ ë„ìš°ë¯¸ RAG ì±—ë´‡")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "vectordb" not in st.session_state:
    st.session_state.vectordb = None
if "sources" not in st.session_state:
    st.session_state.sources = []     # ì´ë²ˆ ì„¸ì…˜ì—ì„œ ì—…ë¡œë“œí•œ ë¬¸ì„œ ì´ë¦„ë“¤
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None

# 1) (ê¸°ì¡´ ì‚¬ì´ë“œë°” â†’ ë©”ì¸ ìƒë‹¨) ì‘ì—… ìœ í˜• ì„ íƒ
st.subheader("1ï¸âƒ£ ì‘ì—… ìœ í˜• ì„ íƒ")

task_mode = st.selectbox(
    "ë„ì›€ ë°›ê³  ì‹¶ì€ ì‘ì—… ìœ í˜•ì„ ë¨¼ì € ì„ íƒí•˜ì„¸ìš”! ì‘ë‹µì˜ í’ˆì§ˆì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    ["ë¬¸ì„œ ìš”ì•½", "ë ˆí¬íŠ¸ ëª©ì°¨ ì„¤ê³„", "í•µì‹¬ ë‚´ìš© ì •ë¦¬", "ì˜ˆìƒ ì‹œí—˜ë¬¸ì œ ìƒì„±", "ììœ  ì§ˆì˜ì‘ë‹µ"],
    index=0,
)

# 2) ë¬¸ì„œ ì—…ë¡œë“œ (ì—¬ëŸ¬ ê°œ, ìµœëŒ€ 5ê°œ)
st.subheader("2ï¸âƒ£ ë¬¸ì„œ ì—…ë¡œë“œ (PDF ë˜ëŠ” TXT, ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)")

uploaded_files = st.file_uploader(
    "ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (ìµœëŒ€ 5ê°œê¹Œì§€ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤)",
    type=["pdf", "txt"],
    accept_multiple_files=True,
)

if uploaded_files:
    new_sources = [f.name for f in uploaded_files]

    if len(new_sources) > MAX_DOCS:
        st.error(
            f"í•œ ë²ˆì— ì—…ë¡œë“œí•  ìˆ˜ ìˆëŠ” ë¬¸ì„œëŠ” ìµœëŒ€ {MAX_DOCS}ê°œì…ë‹ˆë‹¤. "
            f"í˜„ì¬ ì—…ë¡œë“œ ì‹œë„ ë¬¸ì„œ ìˆ˜: {len(new_sources)}ê°œ"
        )
    else:
        # ì´ì „ ì¸ë±ìŠ¤ ì™„ì „íˆ ì œê±°
        if os.path.exists(VECTORSTORE_DIR):
            shutil.rmtree(VECTORSTORE_DIR, ignore_errors=True)

        all_docs = []
        for uf in uploaded_files:
            all_docs.extend(load_and_split_docs(uf))

        st.session_state.vectordb = create_vectorstore(all_docs)
        st.session_state.sources = new_sources

        st.success(
            f"ë¬¸ì„œ {len(new_sources)}ê°œë¥¼ ìƒˆ ë²¡í„°ìŠ¤í† ì–´ë¡œ ìƒì„±í–ˆìŠµë‹ˆë‹¤. "
            "(ì´ì „ ë¬¸ì„œë“¤ì€ ë” ì´ìƒ ì°¸ì¡°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)"
        )

# í˜„ì¬ ì„¸ì…˜ì—ì„œ ì‚¬ìš©í•  source ëª©ë¡
active_sources = st.session_state.sources if st.session_state.sources else []

# 3) RAG ì²´ì¸ êµ¬ì„±
if st.session_state.vectordb is not None and active_sources:
    st.session_state.rag_chain = build_rag_chain(
        st.session_state.vectordb, task_mode, active_sources
    )
else:
    st.session_state.rag_chain = None

# 4) ì§ˆì˜ ì…ë ¥ + ë²„íŠ¼ìœ¼ë¡œ ì¿¼ë¦¬ ì‹¤í–‰
st.subheader("3ï¸âƒ£ ì§ˆë¬¸ ì…ë ¥ ë° ì‹¤í–‰")

if st.session_state.rag_chain:
    question = st.text_area(
        "ì§ˆë¬¸ì´ë‚˜ ì›í•˜ëŠ” ì‘ì—… ë²”ìœ„ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n"
        "ì˜ˆ) 2~3í˜ì´ì§€ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ì¤˜ / í™˜ê²½ì˜¤ì—¼ íŒŒíŠ¸ë§Œ ë ˆí¬íŠ¸ ëª©ì°¨ ì§œì¤˜ / "
        "ì´ ë¬¸ì„œë¡œ ì‹œí—˜ì— ë‚˜ì˜¬ ë²•í•œ ë¬¸ì œ 5ê°œ ë§Œë“¤ì–´ì¤˜ ë“±",
        height=150,
    )

    run_query = st.button("ì¿¼ë¦¬ ì‹¤í–‰")

    if run_query:
        if question.strip():
            with st.spinner("ê³¼ì œ/ë ˆí¬íŠ¸ ë„ì™€ì£¼ëŠ” ì¤‘..."):
                result = st.session_state.rag_chain.invoke({"question": question})
                st.write("### âœï¸ ê²°ê³¼")
                st.write(result.content)
        else:
            st.warning("ë¨¼ì € ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
else:
    st.info("ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
