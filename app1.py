import os
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain_core.runnables import RunnableLambda
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ ì•ˆì— OpenAI API í‚¤ê°€ ì €ì¥ë˜ì–´ ìˆìŒ)
load_dotenv(".env")

# 2. ë²¡í„°ìŠ¤í† ì–´(ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤) ì €ì¥ í´ë” ì„¤ì •
VECTORSTORE_DIR = "faiss_index"

# 3. ë¬¸ì„œ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ë¶„í•  í•¨ìˆ˜
def load_and_split_docs(uploaded_file):
    """
    ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ PDF ë˜ëŠ” TXT ë¬¸ì„œë¥¼ ì½ê³ 
    LangChainì—ì„œ ì²˜ë¦¬ ê°€ëŠ¥í•œ ë¬¸ì„œ ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜.
    - PDF: PyPDFLoader ì‚¬ìš©
    - TXT: TextLoader ì‚¬ìš©
    ì´í›„ RecursiveCharacterTextSplitterë¥¼ ì´ìš©í•´ ì¼ì • ë‹¨ìœ„ë¡œ ë¶„í• í•œë‹¤.
    """
    # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì„ì‹œë¡œ ë¡œì»¬ì— ì €ì¥
    with open(uploaded_file.name, "wb") as f:
        f.write(uploaded_file.getbuffer())

    # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ë‹¤ë¥¸ ë¡œë” ì„ íƒ
    if uploaded_file.name.endswith(".pdf"):
        loader = PyPDFLoader(uploaded_file.name)
    else:
        loader = TextLoader(uploaded_file.name, encoding="utf-8")

    # ë¬¸ì„œ ë¡œë“œ (LangChain Document ê°ì²´ë¡œ ë°˜í™˜ë¨)
    documents = loader.load()

    # ë¬¸ì„œë¥¼ 500ì ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , 100ì ì¤‘ì²©(Overlapping) ì ìš©
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    return splitter.split_documents(documents)


# 4. ë²¡í„°ìŠ¤í† ì–´ ìƒì„± í•¨ìˆ˜ (ìƒˆ ë¬¸ì„œ ì—…ë¡œë“œ ì‹œ ìµœì´ˆ 1íšŒ ì‹¤í–‰)
def create_vectorstore(docs):
    """
    ë¶„í• ëœ ë¬¸ì„œë“¤ì„ OpenAI ì„ë² ë”©ìœ¼ë¡œ ë²¡í„°í™”í•œ í›„,
    FAISS(Vector Store)ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜.
    ì´í›„ ê²€ìƒ‰ì„ ë¹ ë¥´ê²Œ í•˜ê¸° ìœ„í•´ ë¡œì»¬ì— ì €ì¥í•œë‹¤.
    """
    embeddings = OpenAIEmbeddings()                    # OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    vectordb = FAISS.from_documents(docs, embeddings)  # ë¬¸ì„œ ì„ë² ë”© â†’ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
    vectordb.save_local(VECTORSTORE_DIR)               # ë¡œì»¬ í´ë”ì— ì €ì¥
    return vectordb


# 5. ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ í•¨ìˆ˜
def load_vectorstore():
    """
    ì´ë¯¸ ë§Œë“¤ì–´ì§„ FAISS ì¸ë±ìŠ¤ê°€ ë¡œì»¬ì— ì¡´ì¬í•  ê²½ìš° ì´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜.
    ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ None ë°˜í™˜.
    """
    embeddings = OpenAIEmbeddings()
    if os.path.exists(VECTORSTORE_DIR):
        try:
            # langchain ë²„ì „ì— ë”°ë¼ allow_dangerous_deserialization=True ì˜µì…˜ì´
            # í•„ìš”í•  ìˆ˜ ìˆìŒ. ë¬¸ì œ ìƒê¸°ë©´ ì•„ë˜ ì¤„ì²˜ëŸ¼ ìˆ˜ì •:
            return FAISS.load_local(VECTORSTORE_DIR, embeddings, allow_dangerous_deserialization=True)
            #return FAISS.load_local(VECTORSTORE_DIR, embeddings)
        except Exception as e:
            st.warning(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    return None


# 6. RAG (Retrieval-Augmented Generation) ì²´ì¸ êµ¬ì„± í•¨ìˆ˜
def build_rag_chain(vectordb, task_mode: str):
    """
    ê³¼ì œ/ë ˆí¬íŠ¸ ë„ìš°ë¯¸ìš© RAG ì²´ì¸.
    - retriever: ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œ ì¡°ê° ê²€ìƒ‰
    - prompt: ì‘ì—… ìœ í˜•(task_mode)ì— ë”°ë¼ ë‹¤ë¥¸ ì§€ì‹œë¥¼ í¬í•¨
    - llm: ChatOpenAIê°€ ìµœì¢… ë‹µë³€ ìƒì„±
    """
    retriever = vectordb.as_retriever()

    # ê³µí†µ ì‹œìŠ¤í…œ ì§€ì¹¨
    base_instruction = """
    ë„ˆëŠ” ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³¼ì œì™€ ë ˆí¬íŠ¸ ì‘ì„±ì„ ë„ì™€ì£¼ëŠ” AI ì¡°êµì•¼.
    í•­ìƒ ë¬¸ì„œ ë‚´ìš©ì„ ìµœìš°ì„ ìœ¼ë¡œ ì°¸ê³ í•´ì„œ ë‹µí•´ì•¼ í•˜ê³ ,
    ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì´ë¼ ì¼ë°˜ì ì¸ ì„¤ëª…ì„ í• ê²Œ."ë¼ê³  ë¨¼ì € ì•Œë ¤ì¤€ ë’¤ ì„¤ëª…í•´.
    """

    # ì‘ì—… ìœ í˜•ë³„ ì¶”ê°€ ì§€ì¹¨
    if task_mode == "ë¬¸ì„œ ìš”ì•½":
        task_instruction = """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ì§€ì •í•œ ë²”ìœ„ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¬¸ì„œ ë‚´ìš©ì„ 3~7ë¬¸ì¥ ì •ë„ë¡œ ìš”ì•½í•´ì¤˜.
        ì¤‘ìš” ê°œë…, í•µì‹¬ ì£¼ì¥, ê²°ë¡ ì´ ë¹ ì§€ì§€ ì•Šê²Œ ì •ë¦¬í•´.
        """
    elif task_mode == "ë ˆí¬íŠ¸ ëª©ì°¨ ì„¤ê³„":
        task_instruction = """
        ì´ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ A4 3~5ì¥ ë¶„ëŸ‰ì˜ ë ˆí¬íŠ¸ ëª©ì°¨ë¥¼ ì„¤ê³„í•´ì¤˜.
        1, 1-1, 1-2 ì™€ ê°™ì€ ê³„ì¸µ êµ¬ì¡°ë¡œ ì‘ì„±í•˜ê³ ,
        ê° ì†Œì œëª© ì˜†ì— í•œ ì¤„ì”© ê·¸ ë¶€ë¶„ì— ì“°ë©´ ì¢‹ì„ ë‚´ìš©ì„ ì„¤ëª…í•´ì¤˜.
        """
    elif task_mode == "í•µì‹¬ ë‚´ìš© ì •ë¦¬":
        task_instruction = """
        ì´ ë¬¸ì„œì—ì„œ ì‚¬ìš©ìê°€ ê¶ê¸ˆí•´í•˜ëŠ” ì£¼ì œì™€ ê´€ë ¨ëœ í•µì‹¬ ê°œë…, ì£¼ì¥, ê·¼ê±°ë¥¼
        bullet ëª©ë¡ í˜•íƒœë¡œ ì§§ê³  ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì¤˜.
        """
    elif task_mode == "ì˜ˆìƒ ì‹œí—˜ë¬¸ì œ ìƒì„±":
        task_instruction = """
        ì´ ë¬¸ì„œë¥¼ ê³µë¶€í•˜ëŠ” í•™ìƒì—ê²Œ ì‹œí—˜ì´ë‚˜ êµ¬ë‘ ë°œí‘œì—ì„œ ë‚˜ì˜¬ ë²•í•œ ë¬¸ì œë¥¼ 3~5ê°œ ë§Œë“¤ì–´ì¤˜.
        ì„œìˆ í˜•, ê°ê´€ì‹, ë…¼ìˆ í˜• ë“±ì„ ì„ì–´ë„ ì¢‹ê³ ,
        ê° ë¬¸ì œë§ˆë‹¤ ëª¨ë²”ë‹µì•ˆì˜ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ 2~3ì¤„ ì •ë„ë¡œ ì œì‹œí•´ì¤˜.
        """
    else:  # "ììœ  ì§ˆì˜ì‘ë‹µ"
        task_instruction = """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µí•˜ë˜,
        ë°˜ë“œì‹œ ë¬¸ì„œì—ì„œ ê·¼ê±°ê°€ ë˜ëŠ” ë‚´ìš© ìœ„ì£¼ë¡œ ì„¤ëª…í•´ì¤˜.
        """

    prompt = ChatPromptTemplate.from_template(
        base_instruction
        + "\n\n[ì‘ì—… ì§€ì¹¨]\n"
        + task_instruction
        + """
        
        [ì‚¬ìš©ì ì§ˆë¬¸]
        {question}

        [ì°¸ê³  ë¬¸ì„œ ë‚´ìš©]
        {context}
        """
    )

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    # retriever ê²°ê³¼(Document ë¦¬ìŠ¤íŠ¸)ë¥¼ í…ìŠ¤íŠ¸ë¡œ í•©ì³ì£¼ëŠ” ëŒë‹¤
    def join_docs(docs):
        return "\n\n".join([d.page_content for d in docs])

    rag_chain = (
        {
            "context": RunnableLambda(lambda x: x["question"]) 
                       | retriever 
                       | RunnableLambda(lambda docs: join_docs(docs)),
            "question": RunnableLambda(lambda x: x["question"])
        }
        | prompt
        | llm
    )
    return rag_chain


# 7. Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
st.set_page_config(page_title="ê³¼ì œÂ·ë ˆí¬íŠ¸ ë„ìš°ë¯¸ RAG ì±—ë´‡")
st.title("ğŸ“š ê³¼ì œÂ·ë ˆí¬íŠ¸ ë„ìš°ë¯¸ RAG ì±—ë´‡")

st.write("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•œ ë’¤, ì™¼ìª½ì—ì„œ ì‘ì—… ìœ í˜•ì„ ì„ íƒí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ë©ë‹ˆë‹¤.")

# ğŸ”§ ì‚¬ì´ë“œë°”: ì‘ì—… ìœ í˜• ì„ íƒ
st.sidebar.header("ì‘ì—… ìœ í˜• ì„¤ì •")
task_mode = st.sidebar.selectbox(
    "ì–´ë–¤ ë„ì›€ì„ ë°›ê³  ì‹¶ë‚˜ìš”?",
    ["ë¬¸ì„œ ìš”ì•½", "ë ˆí¬íŠ¸ ëª©ì°¨ ì„¤ê³„", "í•µì‹¬ ë‚´ìš© ì •ë¦¬", "ì˜ˆìƒ ì‹œí—˜ë¬¸ì œ ìƒì„±", "ììœ  ì§ˆì˜ì‘ë‹µ"],
    index=0
)

# 8. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "vectordb" not in st.session_state:
    st.session_state.vectordb = None
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None

# 9. ë¡œì»¬ì— ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
vectordb_exists = os.path.exists(VECTORSTORE_DIR)

# 10. ë¬¸ì„œ ì—…ë¡œë“œ UI (PDF, TXT íŒŒì¼ í—ˆìš©)
uploaded_file = st.file_uploader("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (PDF ë˜ëŠ” TXT)", type=["pdf", "txt"])

# 11. ë²¡í„°ìŠ¤í† ì–´ ì¡´ì¬ ì‹œ: ë¡œë“œ í›„ ë°”ë¡œ ì‚¬ìš©
if vectordb_exists and st.session_state.vectordb is None:
    st.session_state.vectordb = load_vectorstore()
    if st.session_state.vectordb:
        st.session_state.rag_chain = build_rag_chain(st.session_state.vectordb, task_mode)
        st.success("ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    else:
        st.warning("ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•˜ì„¸ìš”.")

# 12. ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ì„ ë•Œ: ì—…ë¡œë“œëœ ë¬¸ì„œë¡œ ìƒˆë¡œ ìƒì„±
elif not vectordb_exists:
    if uploaded_file:
        with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ì„ë² ë”© ì¤‘ì…ë‹ˆë‹¤..."):
            split_docs = load_and_split_docs(uploaded_file)              # ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
            st.session_state.vectordb = create_vectorstore(split_docs)   # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            st.session_state.rag_chain = build_rag_chain(
                st.session_state.vectordb, task_mode
            )
            st.success("ìƒˆ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
    else:
        st.info("ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ìœ¼ë¯€ë¡œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.")

# ğŸ”„ ì‘ì—… ìœ í˜•ì´ ë°”ë€Œë©´ ì²´ì¸ì„ ë‹¤ì‹œ êµ¬ì„±
if st.session_state.vectordb is not None:
    st.session_state.rag_chain = build_rag_chain(
        st.session_state.vectordb, task_mode
    )

# 13. ì‚¬ìš©ì ì§ˆì˜ ì…ë ¥ ë° ë‹µë³€ ì¶œë ¥
if st.session_state.rag_chain:
    # ì§ˆë¬¸ ì…ë ¥
    question = st.text_area(
        "ì§ˆë¬¸ì´ë‚˜ ì›í•˜ëŠ” ì‘ì—… ë²”ìœ„ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\nì˜ˆ) 2~3í˜ì´ì§€ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ì¤˜ / í™˜ê²½ì˜¤ì—¼ íŒŒíŠ¸ë§Œ ë ˆí¬íŠ¸ ëª©ì°¨ ì§œì¤˜ ë“±",
        height=120
    )

    # âœ… ì¿¼ë¦¬ ì‹¤í–‰ ë²„íŠ¼
    run_query = st.button("ì¿¼ë¦¬ ì‹¤í–‰")

    if run_query:
        if question.strip():
            with st.spinner("ê³¼ì œ/ë ˆí¬íŠ¸ ë„ì™€ì£¼ëŠ” ì¤‘..."):
                result = st.session_state.rag_chain.invoke({"question": question})
                st.write("### âœï¸ ê²°ê³¼")
                st.write(result.content)
        else:
            st.warning("ë¨¼ì € ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
else:
    st.info("ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³ , ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±/ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.")
