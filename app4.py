import os
import shutil
import streamlit as st

from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda

# -------------------------------
# í™˜ê²½ ì„¤ì •
# -------------------------------
load_dotenv(".env")

VECTORSTORE_DIR = "faiss_index"
MAX_DOCS = 5  # ìµœëŒ€ ì²¨ë¶€ ê°€ëŠ¥ ë¬¸ì„œ ìˆ˜


# -------------------------------
# ìœ í‹¸ í•¨ìˆ˜ë“¤
# -------------------------------
def load_and_split_docs(uploaded_file):
    """
    ì—…ë¡œë“œëœ PDF/TXTë¥¼ ë¡œì»¬ì— ì €ì¥ í›„ LangChain Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³ ,
    RecursiveCharacterTextSplitterë¡œ chunk ë‹¨ìœ„ë¡œ ë¶„í• í•œë‹¤.
    """
    with open(uploaded_file.name, "wb") as f:
        f.write(uploaded_file.getbuffer())

    if uploaded_file.name.endswith(".pdf"):
        loader = PyPDFLoader(uploaded_file.name)
    else:
        loader = TextLoader(uploaded_file.name, encoding="utf-8")

    documents = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    docs = splitter.split_documents(documents)

    # ê° chunkì— source(íŒŒì¼ëª…) ë©”íƒ€ë°ì´í„° ì§€ì •
    for d in docs:
        d.metadata["source"] = uploaded_file.name

    return docs


def create_vectorstore(docs):
    """
    ì£¼ì–´ì§„ ë¬¸ì„œë“¤ë¡œ ìƒˆ FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ê³  ë¡œì»¬ì— ì €ì¥í•œë‹¤.
    (ì´ì „ ì¸ë±ìŠ¤ëŠ” ë°–ì—ì„œ ì´ë¯¸ ì‚­ì œí–ˆë‹¤ê³  ê°€ì •)
    """
    embeddings = OpenAIEmbeddings()
    vectordb = FAISS.from_documents(docs, embeddings)
    vectordb.save_local(VECTORSTORE_DIR)
    return vectordb


def build_rag_chain(vectordb, task_mode: str, active_sources):
    """
    ê³¼ì œ/ë ˆí¬íŠ¸ ë„ìš°ë¯¸ìš© RAG ì²´ì¸.
    active_sources: ì‚¬ìš©í•  ë¬¸ì„œ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
    task_mode:
      - "ë¬¸ì„œ ìš”ì•½" / "ë ˆí¬íŠ¸ ëª©ì°¨ ì„¤ê³„" / "í•µì‹¬ ë‚´ìš© ì •ë¦¬" / "ì˜ˆìƒ ì‹œí—˜ë¬¸ì œ ìƒì„±" 
    """
    base_instruction = """
    ë„ˆëŠ” ì—…ë¡œë“œëœ ë¬¸ì„œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ê³¼ì œì™€ ë ˆí¬íŠ¸ ì‘ì„±ì„ ë„ì™€ì£¼ëŠ” AI ì¡°êµì•¼.
    í•­ìƒ ë¬¸ì„œ ë‚´ìš©ì„ ìµœìš°ì„ ìœ¼ë¡œ ì°¸ê³ í•´ì„œ ë‹µí•´ì•¼ í•˜ê³ ,
    ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì´ë¼ ì¼ë°˜ì ì¸ ì„¤ëª…ì„ í• ê²Œ."ë¼ê³  ë¨¼ì € ì•Œë ¤ì¤€ ë’¤ ì„¤ëª…í•´.
    """

    # ### ë³€ê²½ í¬ì¸íŠ¸ 1: 
    # ë” ì´ìƒ ì—¬ê¸°ì„œ "ì„ íƒ ì•ˆí•¨"ì„ early return í•˜ì§€ ì•ŠìŒ.
    # "ì„ íƒ ì•ˆí•¨"ì¸ ê²½ìš°ì—ëŠ” ì•„ì˜ˆ build_rag_chainì„ í˜¸ì¶œí•˜ì§€ ì•Šë„ë¡ ìƒë‹¨ ë¡œì§ì—ì„œ ì²˜ë¦¬.

    if task_mode == "ë¬¸ì„œ ìš”ì•½":
        task_instruction = """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ì§€ì •í•œ ë²”ìœ„ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¬¸ì„œ ë‚´ìš©ì„ 3~7ë¬¸ì¥ ì •ë„ë¡œ ìš”ì•½í•´ì¤˜.
        ì¤‘ìš” ê°œë…, í•µì‹¬ ì£¼ì¥, ê²°ë¡ ì´ ë¹ ì§€ì§€ ì•Šê²Œ ì •ë¦¬í•´.
        """
    elif task_mode == "ë ˆí¬íŠ¸ ëª©ì°¨ ì„¤ê³„":
        task_instruction = """
        ì´ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ A4 3~5ì¥ ë¶„ëŸ‰ì˜ ë ˆí¬íŠ¸ ëª©ì°¨ë¥¼ ì„¤ê³„í•´ì¤˜.
        1, 1-1, 1-2 ì™€ ê°™ì€ ê³„ì¸µ êµ¬ì¡°ë¡œ ì‘ì„±í•˜ê³ ,
        ê° ì†Œì œëª© ì˜†ì— í•œ ì¤„ì”© ê·¸ ë¶€ë¶„ì— ì“°ë©´ ì¢‹ì„ ë‚´ìš©ì„ ì„¤ëª…í•´ì¤˜.
        """
    elif task_mode == "í•µì‹¬ ë‚´ìš© ì •ë¦¬":
        task_instruction = """
        ì´ ë¬¸ì„œì—ì„œ ì‚¬ìš©ìê°€ ê¶ê¸ˆí•´í•˜ëŠ” ì£¼ì œì™€ ê´€ë ¨ëœ í•µì‹¬ ê°œë…, ì£¼ì¥, ê·¼ê±°ë¥¼
        bullet ëª©ë¡ í˜•íƒœë¡œ ì§§ê³  ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì¤˜.
        """
    elif task_mode == "ì˜ˆìƒ ì‹œí—˜ë¬¸ì œ ìƒì„±":
        task_instruction = """
        ì´ ë¬¸ì„œë¥¼ ê³µë¶€í•˜ëŠ” í•™ìƒì—ê²Œ ì‹œí—˜ì´ë‚˜ êµ¬ë‘ ë°œí‘œì—ì„œ ë‚˜ì˜¬ ë²•í•œ ë¬¸ì œë¥¼ 3~5ê°œ ë§Œë“¤ì–´ì¤˜.
        ì„œìˆ í˜•, ê°ê´€ì‹, ë…¼ìˆ í˜• ë“±ì„ ì„ì–´ë„ ì¢‹ê³ ,
        ê° ë¬¸ì œë§ˆë‹¤ ëª¨ë²”ë‹µì•ˆì˜ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ 2~3ì¤„ ì •ë„ë¡œ ì œì‹œí•´ì¤˜.
        """
    else:  # "ììœ  ì§ˆì˜ì‘ë‹µ" ë“±
        task_instruction = """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µí•˜ë˜,
        ë°˜ë“œì‹œ ë¬¸ì„œì—ì„œ ê·¼ê±°ê°€ ë˜ëŠ” ë‚´ìš© ìœ„ì£¼ë¡œ ì„¤ëª…í•´ì¤˜.
        """

    prompt = ChatPromptTemplate.from_template(
        base_instruction
        + "\n\n[ì‘ì—… ì§€ì¹¨]\n"
        + task_instruction
        + """
        
        [ì‚¬ìš©ì ì§ˆë¬¸]
        {question}

        [ì°¸ê³  ë¬¸ì„œ ë‚´ìš©]
        {context}
        """
    )

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)

    def get_context_from_sources(inputs):
        question = inputs["question"]
        docs = vectordb.similarity_search(question, k=12)

        # active_sourcesê°€ ì§€ì •ë˜ë©´ í•´ë‹¹ sourceë§Œ í•„í„°ë§
        if active_sources:
            docs = [d for d in docs if d.metadata.get("source") in active_sources]

        if not docs:
            return "ì„ íƒí•œ ë¬¸ì„œë“¤ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        return "\n\n".join([d.page_content for d in docs])

    rag_chain = (
        {
            "context": RunnableLambda(get_context_from_sources),
            "question": RunnableLambda(lambda x: x["question"]),
        }
        | prompt
        | llm
    )
    return rag_chain


# -------------------------------
# Streamlit UI
# -------------------------------
st.set_page_config(page_title="ê³¼ì œÂ·ë ˆí¬íŠ¸ ë„ìš°ë¯¸ RAG ì±—ë´‡")
st.title("ğŸ“š ê³¼ì œÂ·ë ˆí¬íŠ¸ ë„ìš°ë¯¸ RAG ì±—ë´‡")

# ìƒë‹¨ ì†Œê°œ ë¬¸êµ¬
st.markdown(
    """
**ì—…ë¡œë“œí•œ PDF/TXT ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³¼ì œÂ·ë³´ê³ ì„œÂ·ìš”ì•½Â·ì‹œí—˜ë¬¸ì œ ì œì‘ì„ ì§€ì›í•˜ëŠ” RAG(ê²€ìƒ‰ ê¸°ë°˜ AI) ì±—ë´‡**ì…ë‹ˆë‹¤.  
ì•„ë˜ ìˆœì„œì— ë”°ë¼ ì§„í–‰í•˜ë©´ ê°€ì¥ ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

1ï¸âƒ£ **ë¬¸ì„œë¥¼ 1~5ê°œ ì—…ë¡œë“œ**í•©ë‹ˆë‹¤. (PDF ë˜ëŠ” TXT)   
2ï¸âƒ£ **ì‘ì—… ëª¨ë“œë¥¼ ì„ íƒ**í•©ë‹ˆë‹¤.  
3ï¸âƒ£ **ì„ íƒí•œ ëª¨ë“œì— ë§ê²Œ ì§ˆë¬¸ ë˜ëŠ” ìš”ì²­ì„ ì…ë ¥(ë˜ëŠ” ìë™ ì‹¤í–‰)**í•˜ë©´, ë¬¸ì„œ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ AIê°€ ì‘ë‹µí•©ë‹ˆë‹¤.  

"""
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "vectordb" not in st.session_state:
    st.session_state.vectordb = None
if "sources" not in st.session_state:
    st.session_state.sources = []     # ì´ë²ˆ ì„¸ì…˜ì—ì„œ ì—…ë¡œë“œí•œ ë¬¸ì„œ ì´ë¦„ë“¤
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None
if "chat_messages" not in st.session_state:
    st.session_state.chat_messages = []   # ëŒ€í™”í˜• ëª¨ë“œìš© ë©”ì‹œì§€ ê¸°ë¡
if "prev_mode" not in st.session_state:
    st.session_state.prev_mode = None
if "last_task_mode" not in st.session_state:
    st.session_state.last_task_mode = None
if "last_task_result" not in st.session_state:
    st.session_state.last_task_result = None

# -------------------------------
# 1ï¸âƒ£ ë¬¸ì„œ ì—…ë¡œë“œ
# -------------------------------
st.subheader("1ï¸âƒ£ ë¬¸ì„œ ì—…ë¡œë“œ (PDF ë˜ëŠ” TXT)")

uploaded_files = st.file_uploader(
    "ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (ìµœëŒ€ 5ê°œê¹Œì§€ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤)",
    type=["pdf", "txt"],
    accept_multiple_files=True,
)

if uploaded_files:
    new_sources = [f.name for f in uploaded_files]

    if len(new_sources) > MAX_DOCS:
        st.error(
            f"í•œ ë²ˆì— ì—…ë¡œë“œí•  ìˆ˜ ìˆëŠ” ë¬¸ì„œëŠ” ìµœëŒ€ {MAX_DOCS}ê°œì…ë‹ˆë‹¤. "
            f"í˜„ì¬ ì—…ë¡œë“œ ì‹œë„ ë¬¸ì„œ ìˆ˜: {len(new_sources)}ê°œ"
        )
    else:
        # ì´ì „ ì¸ë±ìŠ¤ ì™„ì „íˆ ì œê±°
        if os.path.exists(VECTORSTORE_DIR):
            shutil.rmtree(VECTORSTORE_DIR, ignore_errors=True)

        all_docs = []
        for uf in uploaded_files:
            all_docs.extend(load_and_split_docs(uf))

        st.session_state.vectordb = create_vectorstore(all_docs)
        st.session_state.sources = new_sources

        # ê¸°ì¡´ ì‘ì—… ê²°ê³¼/ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.last_task_mode = None
        st.session_state.last_task_result = None

        st.success(
            f"ë¬¸ì„œ {len(new_sources)}ê°œë¥¼ ë¶„ì„ í–ˆìŠµë‹ˆë‹¤. "
        )

# í˜„ì¬ ì„¸ì…˜ì—ì„œ ì‚¬ìš©í•  source ëª©ë¡
active_sources = st.session_state.sources if st.session_state.sources else []

# -------------------------------
# 2ï¸âƒ£ ì‘ì—… ëª¨ë“œ ì„ íƒ
# -------------------------------
st.subheader("2ï¸âƒ£ ì‘ì—… ëª¨ë“œ ì„ íƒ")

mode = st.radio(
    "ì‚¬ìš©í•  ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”.",
    ("ğŸ§© ì›í•˜ëŠ” ì‘ì—…ì„ ì„ íƒí•˜ì„¸ìš”", "ğŸ’¬ ììœ ë¡­ê²Œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”"),
    index=0,
)

is_template_mode = mode.startswith("ğŸ§©")

# ëª¨ë“œê°€ ë³€ê²½ë˜ë©´(íŠ¹íˆ ëŒ€í™”í˜• ëª¨ë“œë¡œ ë“¤ì–´ì˜¬ ë•Œ) ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
if st.session_state.prev_mode != mode:
    if mode.startswith("ğŸ’¬"):
        st.session_state.chat_messages = []
    st.session_state.prev_mode = mode

# -------------------------------
# RAG ì²´ì¸ êµ¬ì„± (ë¬¸ì„œ+ëª¨ë“œ ê¸°ë°˜)
# -------------------------------
task_mode = None

if st.session_state.vectordb is not None and active_sources:
    if is_template_mode:
        task_mode = st.selectbox(
            "ë„ì›€ ë°›ê³  ì‹¶ì€ ì‘ì—… ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”.",
            ["ì„ íƒ ì•ˆí•¨", "ë¬¸ì„œ ìš”ì•½", "ë ˆí¬íŠ¸ ëª©ì°¨ ì„¤ê³„", "í•µì‹¬ ë‚´ìš© ì •ë¦¬", "ì˜ˆìƒ ì‹œí—˜ë¬¸ì œ ìƒì„±"],
            index=0,
        )
    else:
        # ëŒ€í™”í˜• ëª¨ë“œ: ë‚´ë¶€ì ìœ¼ë¡œ 'ììœ  ì§ˆì˜ì‘ë‹µ'
        task_mode = "ììœ  ì§ˆì˜ì‘ë‹µ"

    # ### ë³€ê²½ í¬ì¸íŠ¸ 2:
    # ì‘ì—… ëª¨ë“œê°€ 'ì„ íƒ ì•ˆí•¨'ì´ë©´ RAG ì²´ì¸ì„ ì•„ì˜ˆ ë§Œë“¤ì§€ ì•ŠìŒ
    if task_mode != "ì„ íƒ ì•ˆí•¨":
        st.session_state.rag_chain = build_rag_chain(
            st.session_state.vectordb, task_mode, active_sources
        )
    else:
        st.session_state.rag_chain = None
else:
    st.session_state.rag_chain = None

# -------------------------------
# 3ï¸âƒ£ ëª¨ë“œë³„ ë™ì‘ ì˜ì—­
# -------------------------------
if is_template_mode:
    # -------------------------------
    # ëª¨ë“œ 1: ì‘ì—… í…œí”Œë¦¿ ëª¨ë“œ (ë“œë¡­ë‹¤ìš´ ì„ íƒ ì‹œ ìë™ ì‹¤í–‰)
    # -------------------------------
    # st.subheader("3ï¸âƒ£ ì‘ì—…ì„ íƒ ëª¨ë“œ")

    # ### ë³€ê²½ í¬ì¸íŠ¸ 3:
    # ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ "ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´..." ë¬¸êµ¬ë¥¼ ë³´ì—¬ì¤Œ
    if st.session_state.vectordb is None or not active_sources:
        st.info("ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ ì‘ì—…ì„ íƒ ëª¨ë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        # ë¬¸ì„œëŠ” ì—…ë¡œë“œëœ ìƒíƒœ
        if task_mode == "ì„ íƒ ì•ˆí•¨":
            # ì‘ì—… ëª¨ë“œë¥¼ ì„ íƒí•˜ì§€ ì•Šì€ ê²½ìš°: ê²°ê³¼ ì¶œë ¥ X, ì•ˆë‚´ë§Œ
            st.session_state.last_task_mode = None
            st.session_state.last_task_result = None
            st.info("ìœ„ ë“œë¡­ë‹¤ìš´ì—ì„œ ì›í•˜ëŠ” ì‘ì—… ìœ í˜•ì„ ì„ íƒí•˜ë©´, í•´ë‹¹ ì‘ì—…ì´ ìë™ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        elif st.session_state.rag_chain and task_mode is not None:
            # ì‘ì—… ìœ í˜•ì— ë”°ë¼ ìë™ ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            if task_mode == "ë¬¸ì„œ ìš”ì•½":
                auto_question = "ì´ ë¬¸ì„œ ì „ì²´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ì„ 3~7ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜."
            elif task_mode == "ë ˆí¬íŠ¸ ëª©ì°¨ ì„¤ê³„":
                auto_question = "ì´ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ A4 3~5ì¥ ë¶„ëŸ‰ì˜ ë ˆí¬íŠ¸ ëª©ì°¨ë¥¼ ì„¤ê³„í•´ì¤˜."
            elif task_mode == "í•µì‹¬ ë‚´ìš© ì •ë¦¬":
                auto_question = "ì´ ë¬¸ì„œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í•µì‹¬ ê°œë…, ì£¼ì¥, ê·¼ê±°ë¥¼ ì •ë¦¬í•´ì¤˜."
            elif task_mode == "ì˜ˆìƒ ì‹œí—˜ë¬¸ì œ ìƒì„±":
                auto_question = "ì´ ë¬¸ì„œë¡œ ì‹œí—˜ì´ë‚˜ êµ¬ë‘ ë°œí‘œì—ì„œ ë‚˜ì˜¬ ë²•í•œ ë¬¸ì œë¥¼ 3~5ê°œ ë§Œë“¤ì–´ì¤˜."
            else:  # "ììœ  ì§ˆì˜ì‘ë‹µ" ë“±
                auto_question = "ì´ ë¬¸ì„œì˜ ì „ì²´ ë‚´ìš©ì„ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” í•µì‹¬ ì„¤ëª…ì„ í•´ì¤˜."

            # ì‘ì—…ì´ ë³€ê²½ë˜ì—ˆì„ ë•Œë§Œ ìƒˆë¡œ ì‹¤í–‰
            if task_mode != st.session_state.last_task_mode or st.session_state.last_task_result is None:
                with st.spinner("ì„ íƒí•œ ì‘ì—…ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                    result = st.session_state.rag_chain.invoke({"question": auto_question})
                    st.session_state.last_task_result = result.content
                    st.session_state.last_task_mode = task_mode

            # ê²°ê³¼ í‘œì‹œ
            st.write(f"### âœï¸ ì‘ì—… ê²°ê³¼ â€“ {task_mode}")
            st.write(st.session_state.last_task_result)
        else:
            # ì´ ê²½ìš°ëŠ” ê±°ì˜ ì—†ì§€ë§Œ, ì•ˆì „ë§
            st.info("ì‘ì—… ìœ í˜•ì„ ë‹¤ì‹œ ì„ íƒí•´ ì£¼ì„¸ìš”.")
else:
    # -------------------------------
    # ëª¨ë“œ 2: ëŒ€í™”í˜• ëª¨ë“œ (ì±„íŒ…)
    # -------------------------------
    # st.subheader("3ï¸âƒ£ ëŒ€í™”í˜• ëª¨ë“œ")

    # ê¸°ì¡´ ëŒ€í™” ë‚´ìš© í‘œì‹œ
    for msg in st.session_state.chat_messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # ì±„íŒ… ì…ë ¥ì°½
    user_msg = st.chat_input("ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–´ë–¤ ë„ì›€ì„ ë°›ê³  ì‹¶ë‚˜ìš”?")

    if user_msg:
        # ë¬¸ì„œ/ì²´ì¸ ì¤€ë¹„ ì•ˆ ëì„ ë•Œ
        if st.session_state.rag_chain is None:
            st.warning("ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì•¼ ëŒ€í™”í˜• ëª¨ë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            # ì‚¬ìš©ì ë©”ì‹œì§€ ê¸°ë¡ ë° í™”ë©´ ì¶œë ¥
            st.session_state.chat_messages.append({"role": "user", "content": user_msg})
            with st.chat_message("user"):
                st.markdown(user_msg)

            # RAG í˜¸ì¶œ
            with st.chat_message("assistant"):
                with st.spinner("ìƒê° ì¤‘ì…ë‹ˆë‹¤..."):
                    result = st.session_state.rag_chain.invoke({"question": user_msg})
                    answer = result.content
                    st.markdown(answer)

            # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ê¸°ë¡
            st.session_state.chat_messages.append({"role": "assistant", "content": answer})
